<h1 id="data-dimensionality">Data Dimensionality</h1>
<p>There is a lot of data in the world, and that data has lots of data. Sometimes, it is too much data. Too much for a human to digest surely, but even sometimes too much for a computer to handle in reasonable amounts of time. In these cases, we choose some sort of dimensionality reduction to allow for simpler, quicker calculations.</p>

<p>We do this by not using the original data points ,but combine, transforming, and selecting variables.</p>

<p>It is especially useful in plotting as we can really only see 3 dimensions at one time.</p>

<p>This is also important for aggregating weak signals in data.</p>

<h1 id="principal-component-analysis-and-the-mighty-multivariate-decomposition">Principal Component Analysis and the Mighty Multivariate Decomposition</h1>
<p>Principal Component Analysis is the primary technique for data compression. It relies heavily upon linear algebra and eigenvectors. It works well when variables are linearly correlated.</p>

<p>Goals:
1) Make the data set smaller
2) Finding or revealing hidden</p>

<p>Dimensionality Reduction: Reduces N dimensions down.</p>

<p>Captures the most variation in the data, with fewest dimensions. Variations are “signals” or information in the data.
Discovers variables or dimensions that are highly correlated or redundant, leading to simpler presentation.</p>

<p>Examples when to use it:</p>
<ul>
  <li>Pictures. A simple picture could have millions of pixels, but you can recreate the image with a compressed version that is pretty much the same with only 10% the data load.</li>
  <li>Iris Data Set: classifying flowers based on their petals and sepal characteristics.</li>
</ul>

<p>Good Resources:</p>
<ul>
  <li>[Setosa] (https://setosa.io/ev/principal-component-analysis/)</li>
</ul>

<h1 id="steps">Steps</h1>
<p>1) Estimate Mean and Covariance
2) Take the eigenvectors corresponding to the biggest eigenvalues
3) Compute reduced representation (project your data)</p>

<p>Truncated Singular Value Decomposition (SVD)</p>
<ul>
  <li>Computes eigenvectors</li>
</ul>

<p>Diagnal entries are single values</p>

<h1 id="non-linear-dimensionality-reduction">Non-Linear Dimensionality Reduction</h1>

<p>PCA is really only suitable when variables are linearly correlated.</p>

<p>Examples where PCA might fail:</p>
<ul>
  <li>Two coincentric rings</li>
  <li>Swiss Roll example</li>
</ul>

<p>How do you reduce non-linear dimensions?</p>

<p>First you need a way to measure distance. Since these day sets are based on connectivity, we can define distance as the sum of euclidean distances to get to one point to another, traveling through the data cloud.</p>

<p>This eventually leads to the idea of the Isomap</p>

<h1 id="isomap">Isomap</h1>

<p>Step 1: Build a weighted graph A using nearest neighbors, and compute pairwise shortest distance matrix D.
Step 2: Use a centering matrix $H=I-\frac{1}{M} \cdot1 1^T$ to get a C matrix: $C=-\frac{1}{2m} H D^2 H
Step 3: Compute leading eigenvectors of the C matrix</p>

<p>PCA performs singular value decomposition on the data matrix and eigendecomposition on the covariance matrix.</p>

<p>Check out some isomap results:
[https://www.researchgate.net/figure/The-Isomap-algorithm-when-applied-to-a-synthetic-face-database-of-698-images-The-number_fig5_250747602]
https://benalexkeen.com/isomap-for-dimensionality-reduction-in-python/</p>

<p>Some other good resources:
http://blog.shriphani.com/2014/11/12/the-isomap-algorithm/
https://towardsdatascience.com/step-by-step-signal-processing-with-machine-learning-manifold-learning-8e1bb192461c</p>
