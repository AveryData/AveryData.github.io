<h2 id="correlation-vs-causation">Correlation vs Causation</h2>

<h3 id="correlation">Correlation</h3>
<p>Correlation is a measure of <em>linear relationship between X and Y</em>.</p>

<p>$ Corr(X,Y) = \frac{ um_{i=1}{n} ( (x_i - \bar x) (y_i - \bar y)     )  }   {\sqrt(\Sum_{i=1}{n}(x_i-\bar x)^2) \cdot \sqrt(\Sum_{i=1}{n}(y_i-\bar y)^2)  } $</p>

<p>A function like <script type="math/tex">y = x^2</script> has perfect relation, but a correlation of 0, because non linear.</p>

<h3 id="causation">Causation</h3>
<p>If X and Y are heavily correlated. X could cause Y. Y could cause X. Y could case C which causes X. Or there could be another factor B that influences both.</p>

<p><strong>Correlation does not mean causation!!!!</strong></p>

<p>To get causation, you have to take out all other possible effects.</p>

<h2 id="selection-bias">Selection Bias</h2>
<p>Individuals who are selected for treatment aren’t actually random. Could be because they volunteered, the didn’t volunteer, it was advertised to only one group.</p>

<p>You need to ensure you have a <strong>randomized controlled experiment</strong> that includes treatment and control groups.</p>

<p>Can have <strong>natrual experiments</strong> where the subjects don’t have a choice really. In those experiments, you can see how the response variable changes and calculated <em>difference-in-difference</em> (DID). You basically have a treatment and control group, and a before treatment and after treatment time. It it defined as:
$ DID = \Delta Treated - \Delta Control = (AfterTreated - BeforeTreated) - (AfterControl - BeforeControl) $</p>

<p>This can be used in regression by creating a dummy variable of: Before, After, Control, Treatment. And an interaction term of control, after.</p>

<h2 id="what-is-log-likelihood">What is Log-Likelihood?</h2>
<p>It is the log of the likelihood…what else do you need to know</p>

<p>Let’s start with just likelihood</p>

<p>Usually we have a population that follows a pattern. We usually only have a sample…so our parameters in the sample might not match the population entirely. Hence, we use the best estimate that comes from maximizing the likelihood.</p>

<p><script type="math/tex">Likelihood =  \prod_{i=1}^{N} f(x_i|\theta)</script>
<em>the product of the individual density functions</em></p>

<p>Choose $\theta$ to maximize this function. Well, think about you you find the max of a function? Find the derivative and set it to 0!</p>

<script type="math/tex; mode=display">\frac{\partial L}{\partial \theta} = 0</script>

<p>You’re finding the parameter, $\theta$ that maximizes the log-likelihood. Maximizing the log-likelihood is the same as maximizing the likelihood so why take the log?</p>

<p>Well taking the derivative of products is difficult, you’d need to use the product rule quite a bit. Hence, taking the log makes it easier because in log, you can change products to sum which makes the derivative much easier.</p>
