<h4 id="good-example">Good example</h4>
<p>[https://towardsdatascience.com/how-to-code-gaussian-mixture-models-from-scratch-in-python-9e7975df5252]</p>

<h4 id="good-notebook">Good notebook</h4>
<p>[http://ethen8181.github.io/machine-learning/clustering/GMM/GMM.html]
[https://colab.research.google.com/drive/1PChVghOtJSjWwCYVoevnLGXjnbQNvFOY#scrollTo=cQ2UWSEs_tpF]</p>

<p>GMMs assume all data points are mixture of Gaussian distributions, with unknown parameters.</p>

<p>Itâ€™s actually quite similar to K-means, but unlike K-means, GMMs can learn clusters with any elliptical shape, not just circles. Also, GMMs give probabilities of belonging to a cluster, not hard assignment. This means a point can belong 50% to one cluster and 40% to another.</p>

<p>GMM can also be useful for outlier detection. Points with low likelihoood can be labeled as outliers.</p>
